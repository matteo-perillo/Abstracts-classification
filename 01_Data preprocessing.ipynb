{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "devoted-prototype",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Matteo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Matteo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Matteo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Matteo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Matteo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import scipy\n",
    "\n",
    "#for text pre-processing\n",
    "import re, string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "#for model-building\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "\n",
    "# bag of words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#for word embedding\n",
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "three-receipt",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r data2\n",
    "%store -r cat_extended"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subtle-literature",
   "metadata": {},
   "source": [
    "Text pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bronze-event",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to lowercase, strip and remove punctuations\n",
    "def preprocess(text):\n",
    "    text = text.lower() \n",
    "    text = text.strip()  \n",
    "    text = re.compile('<.*?>').sub('', text) \n",
    "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)  \n",
    "    text = re.sub('\\s+', ' ', text)  \n",
    "    text = re.sub(r'\\[[0-9]*\\]',' ',text) \n",
    "    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "    text = re.sub(r'\\d',' ',text) \n",
    "    text = re.sub(r'\\s+',' ',text) \n",
    "    return text\n",
    "\n",
    " \n",
    "# STOPWORD REMOVAL\n",
    "def stopword(string):\n",
    "    a = [i for i in string.split() if i not in stopwords.words('english')]\n",
    "    return ' '.join(a)\n",
    "\n",
    "\n",
    "#LEMMATIZATION\n",
    "# Initialize the lemmatizer\n",
    "wl = WordNetLemmatizer()\n",
    " \n",
    "# This is a helper function to map NTLK position tags\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "# Tokenize the sentence\n",
    "def lemmatizer(string):\n",
    "    word_pos_tags = nltk.pos_tag(word_tokenize(string)) # Get position tags\n",
    "    a=[wl.lemmatize(tag[0], get_wordnet_pos(tag[1])) for idx, tag in enumerate(word_pos_tags)] # Map the position tag and lemmatize the word/token\n",
    "    return \" \".join(a)\n",
    "\n",
    "def finalpreprocess(string):\n",
    "    return lemmatizer(stopword(preprocess(string)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "matched-cleaner",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r data_toy\n",
    "%store -r data_ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "devoted-eating",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hep-ph      50603\n",
      "quant-ph    44679\n",
      "hep-th      39013\n",
      "gr-qc       25712\n",
      "stat        19868\n",
      "math-ph     17597\n",
      "nucl-th     13730\n",
      "q-bio       13316\n",
      "hep-ex      10158\n",
      "nlin         8934\n",
      "hep-lat      6625\n",
      "q-fin        5979\n",
      "nucl-ex      5735\n",
      "eess         1509\n",
      "econ          246\n",
      "Name: category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data_toy = data2[data2['category'].isin(['hep-ph','hep-th','quant-ph', 'gr-qc', 'stat', 'math-ph', 'nucl-th', 'q-bio',\n",
    "                                         'hep-ex', 'nlin', 'hep-lat', 'q-fin', 'nucl-ex', 'eess', 'econ'])]\n",
    "ind = range(len(data_toy))\n",
    "data_toy = data_toy.set_index(pd.Index(ind))\n",
    "\n",
    "s = time.time()\n",
    "data_toy['clean_text'] = data_toy['abstract'].apply(lambda x: finalpreprocess(x))\n",
    "f = time.time()\n",
    "print(f-s)\n",
    "\n",
    "data_ready = pd.DataFrame({\"clean_text\": data_toy['clean_text'], \"category\": data_toy['category']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "designing-jenny",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'data_toy' (DataFrame)\n",
      "Stored 'data_ready' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "%store data_toy\n",
    "%store data_ready"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moving-reggae",
   "metadata": {},
   "source": [
    "Extracting vectors from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "attempted-september",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPLITTING THE TRAINING DATASET INTO TRAIN AND TEST\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_ready[\"clean_text\"], data_ready[\"category\"],\n",
    "                                                    stratify = data_ready[\"category\"], test_size=0.2, shuffle=True)\n",
    "\n",
    "#Tf-Idf\n",
    "tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n",
    "X_train = tfidf_vectorizer.fit_transform(X_train) \n",
    "X_test = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "regulated-appliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.sort_indices()\n",
    "X_test.sort_indices()\n",
    "\n",
    "y_train.reset_index(inplace=True, drop=True)\n",
    "y_test.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dried-budget",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'X_train_tfidf' (csr_matrix)\n",
      "Stored 'y_train' (Series)\n",
      "Stored 'X_test_tfidf' (csr_matrix)\n",
      "Stored 'y_test' (Series)\n"
     ]
    }
   ],
   "source": [
    "%store X_train_tfidf\n",
    "%store y_train\n",
    "%store X_test_tfidf\n",
    "%store y_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
